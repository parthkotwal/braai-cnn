{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zB7-Fqesfs3"
   },
   "source": [
    "### Model Structure\n",
    "| Layer             |\n",
    "| ----------------- |\n",
    "| `Conv2D(16, 3x3)` |\n",
    "| `Conv2D(16, 3x3)` |\n",
    "| `MaxPool2D(2x2)`  |\n",
    "| `Dropout`         |\n",
    "| `Conv2D(32, 3x3)` |\n",
    "| `Conv2D(32, 3x3)` |\n",
    "| `MaxPool2D(4x4)`  |\n",
    "| `Dropout`         |\n",
    "| `Flatten`         |\n",
    "| `Dense(256)`      |\n",
    "| `Dropout`         |\n",
    "| `Dense(1)`        |\n",
    "\n",
    "![BRAAI CNN Model Structure](images/fig-braai.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3UKNjfkvdgk",
    "outputId": "e53b70a5-4d38-4713-c1b7-8b14f8784e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vu7MVdqpvBr",
    "outputId": "4774cb30-0e6a-4816-b5ad-c6a1aa0a6149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cupy._io.npz.NpzFile at 0x7fb80dfe5bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cupy as np\n",
    "\n",
    "dataset_dir = \"/content/drive/MyDrive/braai_cnn/\"\n",
    "dataset_path = dataset_dir + \"ztf_dataset_split.npz\"\n",
    "data = np.load(dataset_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bD-xSUn-vxOZ",
    "outputId": "4a75a707-3b7c-4b84-8b2f-2565e78cdc19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS                           : Linux-6.1.123+-x86_64-with-glibc2.35\n",
      "Python Version               : 3.11.13\n",
      "CuPy Version                 : 13.3.0\n",
      "CuPy Platform                : NVIDIA CUDA\n",
      "NumPy Version                : 2.0.2\n",
      "SciPy Version                : 1.15.3\n",
      "Cython Build Version         : 0.29.36\n",
      "Cython Runtime Version       : 3.0.12\n",
      "CUDA Root                    : /usr/local/cuda\n",
      "nvcc PATH                    : /usr/local/cuda/bin/nvcc\n",
      "CUDA Build Version           : 12060\n",
      "CUDA Driver Version          : 12040\n",
      "CUDA Runtime Version         : 12060 (linked to CuPy) / 12050 (locally installed)\n",
      "CUDA Extra Include Dirs      : ['/usr/local/lib/python3.11/dist-packages/nvidia/cuda_runtime/include']\n",
      "cuBLAS Version               : (available)\n",
      "cuFFT Version                : 11203\n",
      "cuRAND Version               : 10306\n",
      "cuSOLVER Version             : (11, 6, 3)\n",
      "cuSPARSE Version             : (available)\n",
      "NVRTC Version                : (12, 5)\n",
      "Thrust Version               : 200600\n",
      "CUB Build Version            : 200600\n",
      "Jitify Build Version         : <unknown>\n",
      "cuDNN Build Version          : (not loaded; try `import cupy.cuda.cudnn` first)\n",
      "cuDNN Version                : (not loaded; try `import cupy.cuda.cudnn` first)\n",
      "NCCL Build Version           : 21602\n",
      "NCCL Runtime Version         : 22203\n",
      "cuTENSOR Version             : None\n",
      "cuSPARSELt Build Version     : None\n",
      "Device 0 Name                : NVIDIA L4\n",
      "Device 0 Compute Capability  : 89\n",
      "Device 0 PCI Bus ID          : 0000:00:03.0\n"
     ]
    }
   ],
   "source": [
    "np.show_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoS3iWnZbtnh",
    "outputId": "22a0a700-8ed9-4b42-b649-5f5e383f9d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul 11 18:03:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
      "| N/A   46C    P0             17W /   72W |     189MiB /  23034MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3mnUVdJZqxJk",
    "outputId": "73ec1687-99b0-4a59-a34d-ee0d3c7b3efd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28776, 3, 63, 63), (28776,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[\"X_train\"]\n",
    "y = data[\"y_train\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iaOLKQtc1XaD"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t = 1000 * time.time()\n",
    "np.random.seed(int(t) % 2**30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HPAkpqB_rzM_"
   },
   "outputs": [],
   "source": [
    "# Layer inteface\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, d_out, lr):\n",
    "        pass\n",
    "\n",
    "    def get_parameters(self):\n",
    "        params = []\n",
    "        if hasattr(self, 'kernels'):\n",
    "            params.append(self.kernels)\n",
    "        if hasattr(self, 'biases'):\n",
    "            params.append(self.biases)\n",
    "        if hasattr(self, 'weights'):\n",
    "            params.append(self.weights)\n",
    "        return params\n",
    "\n",
    "    def get_gradients(self):\n",
    "        grads = []\n",
    "        if hasattr(self, 'kernels_grad'):\n",
    "            grads.append(self.kernels_grad)\n",
    "        if hasattr(self, 'biases_grad'):\n",
    "            grads.append(self.biases_grad)\n",
    "        if hasattr(self, 'weights_grad'):\n",
    "            grads.append(self.weights_grad)\n",
    "        return grads\n",
    "\n",
    "\n",
    "    def set_parameters(self, params):\n",
    "        i = 0\n",
    "        if hasattr(self, 'kernels'):\n",
    "            self.kernels = params[i]\n",
    "            i += 1\n",
    "        if hasattr(self, 'biases'):\n",
    "            self.biases = params[i]\n",
    "            i += 1\n",
    "        if hasattr(self, 'weights'):\n",
    "            self.weights = params[i]\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UVGR6QGIdIVo"
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, params, lr, beta1, beta2, epsilon):\n",
    "        self.params = params\n",
    "        self.m = [np.zeros_like(param) for param in params]\n",
    "        self.v = [np.zeros_like(param) for param in params]\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, grads):\n",
    "        self.t += 1\n",
    "        for i in range(len(self.params)):\n",
    "            g = grads[i]\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1-self.beta1)*g\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1-self.beta2)*(g**2)\n",
    "\n",
    "            m_hat = self.m[i] / (1 - self.beta1**self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2**self.t)\n",
    "\n",
    "            self.params[i] -= self.lr * (m_hat / (self.epsilon + np.sqrt(v_hat)))\n",
    "\n",
    "    def collect_params(self):\n",
    "        return self.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Yd5bUMcjqLAQ"
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "from cupy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "# Converts NCHW input to column matrix\n",
    "def im2col(input, KH, KW, stride=1):\n",
    "      N, C, H, W = input.shape\n",
    "      OH = (H - KH) // stride + 1\n",
    "      OW = (W - KW) // stride + 1\n",
    "\n",
    "      patches = sliding_window_view(input, (KH, KW), axis=(2,3))\n",
    "      patches = patches[:, :, ::stride, ::stride, :, :]\n",
    "      cols = patches.transpose(0,2,3,1,4,5).reshape(N*OH*OW, -1)\n",
    "      return cols, OH, OW\n",
    "\n",
    "# Reverse of im2col for accumulating gradients to x\n",
    "def col2im(cols, input_shape, KH, KW, stride=1):\n",
    "    N, C, H, W = input_shape\n",
    "    OH = (H - KH) // stride + 1\n",
    "    OW = (W - KW) // stride + 1\n",
    "\n",
    "    patches = cols.reshape(N, OH, OW, C, KH, KW).transpose(0, 3, 4, 5, 1, 2)\n",
    "    x_grad = np.zeros(input_shape, dtype=cols.dtype)\n",
    "\n",
    "    for i in range(KH):\n",
    "        for j in range(KW):\n",
    "            x_grad[:, :, i:i+stride*OH:stride, j:j+stride*OW:stride] += patches[:, :, i, j]\n",
    "    return x_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aq61XrPMtmUJ"
   },
   "outputs": [],
   "source": [
    "from cupyx.scipy.signal import correlate2d, convolve2d\n",
    "\n",
    "class Conv2D(Layer):\n",
    "    def __init__(self, kernel_size, output_depth):\n",
    "        self.output_depth = output_depth\n",
    "        self.kernel_size = kernel_size\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize_layer(self, input_shape):\n",
    "        N, self.input_depth, H, W = input_shape\n",
    "        self.KH = self.kernel_size\n",
    "        self.KW = self.kernel_size\n",
    "        self.OH = H - self.KH + 1\n",
    "        self.OW = W - self.KW + 1\n",
    "\n",
    "        fan_in = self.input_depth * self.KH * self.KW\n",
    "        self.kernels = np.random.randn(self.output_depth, self.input_depth, self.KH, self.KW) * np.sqrt(2.0 / fan_in)\n",
    "        self.biases = np.random.randn(self.output_depth)\n",
    "        self.initialized = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        if not self.initialized:\n",
    "            self.initialize_layer(input.shape)\n",
    "\n",
    "        self.input = input\n",
    "        # 1. im2 col: all patches in one big matrix\n",
    "        cols = im2col(input, self.KH, self.KW)[0]\n",
    "\n",
    "        # 2. reshape kernels to 2D\n",
    "        W_col = self.kernels.reshape(self.output_depth, -1)\n",
    "\n",
    "        # 3. GEMM\n",
    "        out_cols = cols @ W_col.T\n",
    "        out_cols += self.biases\n",
    "\n",
    "        # 4. Reshape back to OG\n",
    "        N = input.shape[0]\n",
    "        self.output = out_cols.reshape(N, self.OH, self.OW, self.output_depth).transpose(0,3,1,2)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out, lr):\n",
    "        N = d_out.shape[0]\n",
    "\n",
    "        # 1. flatten d_out to columns\n",
    "        d_cols = d_out.transpose(0,2,3,1).reshape(-1, self.output_depth)\n",
    "\n",
    "        # 2. grad kernels\n",
    "        cols = im2col(self.input, self.KH, self.KW)[0]\n",
    "        W_col_grad = d_cols.T @ cols\n",
    "        self.kernels_grad = W_col_grad.reshape(self.kernels.shape) / N\n",
    "\n",
    "        # 3. grad input cols\n",
    "        W_col = self.kernels.reshape(self.output_depth, -1)\n",
    "        cols_grad = d_cols @ W_col\n",
    "\n",
    "        # 4. col2im to get input gradient\n",
    "        input_grad = col2im(cols_grad, self.input.shape, self.KH, self.KW)\n",
    "\n",
    "        # 5) bias grad\n",
    "        self.biases_grad = d_out.sum(axis=(0,2,3)) / N\n",
    "        return input_grad\n",
    "\n",
    "    def get_gradients(self):\n",
    "      return [self.kernels_grad, self.biases_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uaAoRPaDs0aC"
   },
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def backward(self, d_out, lr):\n",
    "        # Relu prime\n",
    "        # print(f\"{self.__class__.__name__} received grad of shape {d_out.shape}\")\n",
    "\n",
    "        return d_out * (self.input > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c6UDnwXYs2nn"
   },
   "outputs": [],
   "source": [
    "class Flatten(Layer):\n",
    "    def forward(self, input):\n",
    "        self.input_shape = input.shape # (N, C, H, W)\n",
    "        return input.reshape(self.input_shape[0], -1) # (N, C*H*W)\n",
    "\n",
    "    def backward(self, d_out, lr=None):\n",
    "        # print(f\"{self.__class__.__name__} received grad of shape {d_out.shape}\")\n",
    "\n",
    "        return d_out.reshape(self.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VNiXS4T8s39h"
   },
   "outputs": [],
   "source": [
    "class MaxPool2D(Layer):\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        N, C, H, W = input.shape\n",
    "\n",
    "        patches, OH, OW = im2col(input, self.pool_size, self.pool_size, stride=self.stride)\n",
    "        patches = patches.reshape(-1, C, self.pool_size*self.pool_size)\n",
    "        self.argmax = patches.argmax(axis=2)\n",
    "\n",
    "        out = patches.max(axis=2)\n",
    "        out = out.reshape(N, OH, OW, C).transpose(0,3,1,2)\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out, lr=None):\n",
    "        N, C, H, W = self.input.shape\n",
    "        # print(d_out.shape)\n",
    "        dum1, dum2, OH, OW = d_out.shape\n",
    "\n",
    "        d_flat = d_out.transpose(0,2,3,1).reshape(-1, C)\n",
    "        patches_grad = np.zeros((d_flat.shape[0], C, self.pool_size*self.pool_size), dtype=d_flat.dtype)\n",
    "\n",
    "        i = np.arange(patches_grad.shape[0])[:,None]\n",
    "        patches_grad[i, np.arange(C), self.argmax] = d_flat\n",
    "\n",
    "        cols_grad = patches_grad.reshape(-1, C*self.pool_size*self.pool_size)\n",
    "        return col2im(cols_grad, self.input.shape, self.pool_size, self.pool_size, stride=self.stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_JDhhMcOs5IB"
   },
   "outputs": [],
   "source": [
    "class FC(Layer):\n",
    "    def __init__(self, input_size=None, output_size=None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "\n",
    "        if self.weights is None and self.output_size is not None:\n",
    "            self.input_size = input.shape[-1]\n",
    "            # self.weights = np.random.randn(self.output_size, self.input_size)\n",
    "            limit = np.sqrt(6 / (self.input_size + self.output_size))\n",
    "            self.weights = np.random.uniform(-limit, limit, (self.output_size, self.input_size))\n",
    "            self.biases = np.zeros(self.output_size)\n",
    "\n",
    "        if input.ndim == 1:\n",
    "            # print(\"Final FC output before sigmoid:\", input[:5].flatten())\n",
    "            return np.dot(self.weights, input) + self.biases\n",
    "        else:\n",
    "            # print(\"Final FC output before sigmoid:\", input[:5].flatten())\n",
    "\n",
    "            return np.dot(input, self.weights.T) + self.biases\n",
    "\n",
    "    def backward(self, d_out, lr):\n",
    "        # print(f\"[FC backward] d_out.shape={d_out.shape}, input.shape={self.input.shape}, weights.shape={self.weights.shape}\")\n",
    "        if self.input.ndim == 1 and self.weights is not None:\n",
    "            weights_gradient = np.outer(d_out, self.input)\n",
    "            input_gradient = np.dot(self.weights.T, d_out)\n",
    "        else:\n",
    "            weights_gradient = np.dot(d_out.T, self.input) / d_out.shape[0]\n",
    "            input_gradient = np.dot(d_out, self.weights)\n",
    "\n",
    "        self.weights_grad = weights_gradient\n",
    "        self.biases_grad = d_out.mean(axis=0) if d_out.ndim > 1 else d_out\n",
    "\n",
    "        return input_gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3ST7RVd3s65-"
   },
   "outputs": [],
   "source": [
    "class Dropout(Layer):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.training = True\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            self.mask = (np.random.rand(*input.shape) > self.rate).astype(input.dtype)\n",
    "            return input * self.mask / (1.0 - self.rate)\n",
    "        else:\n",
    "            return input\n",
    "\n",
    "    def backward(self, d_out, lr=None):\n",
    "        if self.training:\n",
    "            return d_out * self.mask / (1.0 - self.rate)\n",
    "        else:\n",
    "            return d_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JQQ_WsXus8b9"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def forward(self, input):\n",
    "        self.input = np.clip(input, -500, 500)\n",
    "        self.out = 1 / (1 + np.exp(-self.input))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, d_out, lr=None):\n",
    "        # print(f\"{self.__class__.__name__} received grad of shape {d_out.shape}\")\n",
    "        return d_out * self.out * (1-self.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "SmNFLyJ_s9qO"
   },
   "outputs": [],
   "source": [
    "def BCE(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1-(1e-15))\n",
    "    return -(y_true*np.log(y_pred) + (1-y_true) * np.log(1-y_pred))\n",
    "\n",
    "def BCEG(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1-(1e-15))\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "RTztDH2atAJx"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "class NumpyCNN:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            # Block 1\n",
    "            Conv2D(kernel_size=3, output_depth=16), ReLU(),\n",
    "            Conv2D(kernel_size=3, output_depth=16), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=2),\n",
    "            Dropout(dropout_rate=0.25),\n",
    "\n",
    "            # Block 2\n",
    "            Conv2D(kernel_size=3, output_depth=32), ReLU(),\n",
    "            Conv2D(kernel_size=3, output_depth=32), ReLU(),\n",
    "            MaxPool2D(pool_size=2, stride=4),\n",
    "            Dropout(dropout_rate=0.25),\n",
    "\n",
    "            # Fully connected\n",
    "            Flatten(),\n",
    "            FC(input_size=1152, output_size=256), ReLU(),\n",
    "            Dropout(dropout_rate=0.5),\n",
    "            FC(input_size=256, output_size=1), Sigmoid()\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Dropout):\n",
    "                layer.training = True\n",
    "\n",
    "            # print(f\"Before layer #{i+1} {layer.__class__.__name__}: Input shape is {x.shape}\")\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad, lr):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad, lr)\n",
    "        return grad\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                layer.training = False\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, filename):\n",
    "        save_dict = {}\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ps = layer.get_parameters()\n",
    "            cpu_ps = [np.asnumpy(p) for p in ps]\n",
    "            for j, p in enumerate(cpu_ps):\n",
    "                save_dict[f\"layer_{i}_param_{j}\"] = p\n",
    "        numpy.savez(filename, **save_dict)\n",
    "        print(f\"Saved model at {filename}\")\n",
    "\n",
    "\n",
    "    def load(self, filename, input_shape):\n",
    "        dummy_input = np.zeros(input_shape)\n",
    "        self.predict(dummy_input)\n",
    "\n",
    "        data = numpy.load(filename, allow_pickle=True)\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            ps = []\n",
    "            j = 0\n",
    "            while True:\n",
    "                key = f\"layer_{i}_param_{j}\"\n",
    "                if key in data:\n",
    "                    ps.append(np.array(data[key]))\n",
    "                    j += 1\n",
    "                else:\n",
    "                    break\n",
    "            if ps:\n",
    "                layer.set_parameters(ps)\n",
    "\n",
    "\n",
    "        print(f\"Model from {filename} loaded into {type(self).__name__}\")\n",
    "\n",
    "\n",
    "    def collect_parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.get_parameters())\n",
    "        return params\n",
    "\n",
    "    def collect_gradients(self):\n",
    "        grads = []\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, \"get_gradients\"):\n",
    "                grads.extend(layer.get_gradients())\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NOr8Eb-stCOy"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "def train(model, X_train, y_train, epochs, lr, batch_size=16):\n",
    "    n = len(X_train)\n",
    "    optimizer = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for start in range(0, n, batch_size):\n",
    "            # start_time = time.time()\n",
    "            end = start + batch_size\n",
    "            batch_x = X_train[start:end]\n",
    "            batch_y = y_train[start:end].reshape(-1, 1)\n",
    "\n",
    "            out = model.forward(batch_x)\n",
    "\n",
    "            if optimizer is None:\n",
    "                optimizer = Adam(\n",
    "                    model.collect_parameters(),\n",
    "                    lr=lr,\n",
    "                    beta1=0.9,\n",
    "                    beta2=0.999,\n",
    "                    epsilon=1e-8\n",
    "                )\n",
    "\n",
    "            loss = BCE(batch_y, out).mean()\n",
    "            grad = BCEG(batch_y, out)\n",
    "\n",
    "            total_loss += loss * batch_y.shape[0]\n",
    "            model.backward(grad, lr=None)\n",
    "            grads = model.collect_gradients()\n",
    "            optimizer.step(grads)\n",
    "\n",
    "            preds = (out > 0.5).astype(int).flatten()\n",
    "            batch_y_flat = batch_y.flatten()\n",
    "            correct += np.sum(preds == batch_y_flat)\n",
    "            # print(time.time()-start_time)\n",
    "\n",
    "\n",
    "        accuracy = correct / n\n",
    "        avg_loss = total_loss / n\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} — Loss: {avg_loss:.4f} — Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WOkd29SYtFps",
    "outputId": "2dcf58f2-1a8e-4164-ce4a-f4b8029fdce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 — Loss: 0.6844 — Accuracy: 0.5701\n",
      "Epoch 2/20 — Loss: 0.6024 — Accuracy: 0.6584\n",
      "Epoch 3/20 — Loss: 0.5674 — Accuracy: 0.6948\n",
      "Epoch 4/20 — Loss: 0.5453 — Accuracy: 0.7162\n",
      "Epoch 5/20 — Loss: 0.5303 — Accuracy: 0.7262\n",
      "Epoch 6/20 — Loss: 0.5114 — Accuracy: 0.7439\n",
      "Epoch 7/20 — Loss: 0.5021 — Accuracy: 0.7495\n",
      "Epoch 8/20 — Loss: 0.4858 — Accuracy: 0.7615\n",
      "Epoch 9/20 — Loss: 0.4766 — Accuracy: 0.7671\n",
      "Epoch 10/20 — Loss: 0.4660 — Accuracy: 0.7721\n",
      "Epoch 11/20 — Loss: 0.4625 — Accuracy: 0.7744\n",
      "Epoch 12/20 — Loss: 0.4513 — Accuracy: 0.7813\n",
      "Epoch 13/20 — Loss: 0.4478 — Accuracy: 0.7841\n",
      "Epoch 14/20 — Loss: 0.4419 — Accuracy: 0.7872\n",
      "Epoch 15/20 — Loss: 0.4355 — Accuracy: 0.7910\n",
      "Epoch 16/20 — Loss: 0.4348 — Accuracy: 0.7923\n",
      "Epoch 17/20 — Loss: 0.4227 — Accuracy: 0.7991\n",
      "Epoch 18/20 — Loss: 0.4246 — Accuracy: 0.7984\n",
      "Epoch 19/20 — Loss: 0.4173 — Accuracy: 0.8052\n",
      "Epoch 20/20 — Loss: 0.4168 — Accuracy: 0.8015\n"
     ]
    }
   ],
   "source": [
    "# np.cuda.runtime.deviceSynchronize()\n",
    "model = NumpyCNN()\n",
    "\n",
    "train(model, X, y, epochs=20, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77oZkq8xy0C2"
   },
   "outputs": [],
   "source": [
    "filename = \"/content/drive/MyDrive/braai_cnn/numpy_cnn.npz\"\n",
    "model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
